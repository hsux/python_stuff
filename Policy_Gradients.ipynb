{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradients\n",
    "https://morvanzhou.github.io/tutorials/machine-learning/reinforcement-learning/5-1-A-PG/\n",
    "\n",
    " Q learning, Deep Q Network都是学习奖惩值，根据自己认为的高价值选行为；如果在无穷多的动作中计算价值吃不消。\n",
    " \n",
    " Policy Gradients 直接输出行为，可以在一个连续区间内挑选动作。没有误差反向传递，但有反向传递，为了让这次被选中的行为更有可能在下次发生。使用reward奖惩确定这个行为是否应当被增加选中的概率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy gradients核心思想举例：设有2个action，假如这次的观测信息让神经网络选择了 action 1 ,  action 1 随之想要进行反向传递, 使 action 1 下次被多选一点, 这时, 奖惩信息也来了, 告诉我们这是好行为, 那我们就在这次反向传递的时候加大力度, 让它下次被多选的幅度更猛烈! 假如观测的信息通过神经网络分析, 选出了 action 2 , 我们直接进行反向传递, 使之下次被选的可能性增加, 但是奖惩信息却告诉我们,  action 2 是不好的, 那我们的动作可能性增加的幅度随之被减低. 这样就能靠奖励来左右我们的神经网络反向传递."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradients 算法更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
